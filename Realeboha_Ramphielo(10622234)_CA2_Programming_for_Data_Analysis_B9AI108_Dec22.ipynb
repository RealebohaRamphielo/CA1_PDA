{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RealebohaRamphielo/CA1_PDA/blob/main/Realeboha_Ramphielo(10622234)_CA2_Programming_for_Data_Analysis_B9AI108_Dec22.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wcyWEToIFQIF"
      },
      "source": [
        "<h1><b>Realeboha Raymond Ramphielo (10622234)</b></h1>\n",
        "\n",
        "<h3><i> B9AI108 Programming For Data Analysis</h3>\n",
        "<h3> CA 2 </i></h3>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<ul type=disk>\n",
        "<li><a href=\"https://github.com/RealebohaRamphielo/CA1_PDA\"> GitHub link </a></li>\n",
        "<li><a href=\"https://colab.research.google.com/drive/1XIZa4PnihdmvbuFt9s89LH5AF3aYZIFr?usp=sharing\"> Colab Link </a></li>\n",
        "</ul>\n",
        "<p align=justify>Please to open in jupyter because colab might bring some issues when getting/ fetching the url. The project is hosted in colab, so regular updates are on colab, a copy was saved to GitHub for backup purposes, no real work was done on GitHub</p>"
      ],
      "metadata": {
        "id": "Pu8t0o2xGI8W"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CzRU3vnhrTPc"
      },
      "source": [
        "## **Intoduction**\n",
        "<p align=justify> Job scraping is the application of web scraping to employment. Users can easily collect employment information and build a meaningful job database using this automated online data extraction technique. There are several websites to fetch job data, these range from big job boards like <i>Irishjobs.ie</i> and the likes, right down to individual companiesâ€™ career portals. Jobs scraping involves gathering information regarding jobs and downloading it into structured and usable formats, like CSV, that can be used for analysis (Gangadhara Reddy and Viswanath, 2022). An obvious advantage to job scraping is probably data relevancy and timeliness. I will capture the HTML text using the standard Requests and Beautiful Soup configurations, and then convert it to a Beautiful Soup object to simply parse through it and retrieve the data points. I plan to scrap the Career Junction website (South African). From the website, I will scrap the Job title, copany, location, salary, day of posting, day of expiry, and a link to the job. </p>  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H8Lsgwq6HBDB"
      },
      "source": [
        "## Libarary intallations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eb1fYUPb_eB7",
        "outputId": "44636d85-b701-4c2e-d448-da29b876ad7b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (2.23.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests) (2022.9.24)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests) (3.0.4)\n"
          ]
        }
      ],
      "source": [
        "pip install requests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WaNrME8M_irW",
        "outputId": "ad2b2ff0-812c-4c6b-9a5e-530d28ff1b7a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: bs4 in /usr/local/lib/python3.8/dist-packages (0.0.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.8/dist-packages (from bs4) (4.6.3)\n"
          ]
        }
      ],
      "source": [
        "pip install bs4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ehHDOAutHXw"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import json\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "from datetime import datetime, date, timedelta \n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pdHx1dJR47i8"
      },
      "source": [
        "## **Steps**\n",
        "<ol>\n",
        "<li>Getting the url</li>\n",
        "<li>Scaling the salary</li>\n",
        "<li>Get the record</li>\n",
        "<li>Making the soup</li>\n",
        "<li>Creating the dataframe and csv</li>\n",
        "<li>Attempt to wrap inside a class</li>\n",
        "</ol>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qRw6Wd8rkmol"
      },
      "source": [
        "### ***Getting the url*** \n",
        "<p align=justify> The get_url function will accept two string parameters; job and location, location is set at default value of 0 which translates to all locations. Within this function is a dictionary variable, locations. Locations will convert/ encode the second parameter into a usable location code by pointing to the location's value in the locations deictionary. After encoding the location, the next step is to 'plug' the desired job title and encoded location into the url and create a url that will be returned by the function. In a case where the passed location is not defined, a KeyValueError will be raised by the function and an error message will be returned. </p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xpm-xKLhr1Qp"
      },
      "outputs": [],
      "source": [
        "def get_url(job, location = 0):\n",
        "  \n",
        "  \"\"\"Builds the URL based on job title and location\n",
        "\n",
        "  :param job: job title to search for\n",
        "  :param location: location to search at\n",
        "  :return: the complete URL with job and location\n",
        "\n",
        "  >>>get_url('data', 'limpopo')\n",
        "  https://www.careerjunction.co.za/jobs/results?keywords=data&autosuggest\\\n",
        "  Endpoint=%2Fautosuggest&location=14594&category=&btnSubmit=+\"\n",
        "  \"\"\"\n",
        "\n",
        "  #When no location is specified, default location = 0, i.e. the search will not\n",
        "  # have any location restrictions\n",
        "\n",
        "  locations = {'Gauteng':'2747', 'Western Cape':'16149', 'KwaZulu-Natal':'13131',\\\n",
        "              'Eastern Cape':'2', 'Free State':'1782', 'Mpumalanga':'14867',\\\n",
        "              'Limpopo':'14594', 'North West':'15372', 'Northern Cape':'15837',\\\n",
        "              'International':'1000001', 'Working From Home':'100000'}\n",
        "\n",
        "  #above is the locations dictionary that will encode the location from a normal\n",
        "  #word search to an 'int' search\n",
        "  \n",
        "  error_message = 'No access to location, please revise'\n",
        "  \n",
        "  try:\n",
        "    if location != 0:                    #encode the location\n",
        "      location = locations[location.title()]  #.title() helps to capitalise every first letter for proper formatting\n",
        "    gen_url = \"https://www.careerjunction.co.za/jobs/results?keywords={}&autosuggestEndpoint=%2Fautosuggest&location={}&category=&btnSubmit=+\"\n",
        "    search_url = gen_url.format(job, location)\n",
        "    \n",
        "    return search_url\n",
        "  \n",
        "  except KeyError:\n",
        "    return error_message"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AZTFv6xDrXq-"
      },
      "source": [
        "###***Scaling the salary***\n",
        "<p align=justify> The salary_convert funtion will take one string parameter. The purpose of this function is to scale the salary to monthly rates. It will first strip the string off the 'R' which denotes the ZAR (South AFrican Rand), then strip off any white and split the string using white space. Following the split, we will take the first element (element at index 0) and follow this by splitting it using the period (dot) character and assign to a_string variable the first element from the resulting split. Some figures are comma separated, thus the second step will be to remove any commas from a_string and covert the variable to a float and multiply it by the appropriate scaling factor to calculate a monthly salary, the scaling factors are as follows:\n",
        "<ul>\n",
        "<li>160 for hourly pay</li>\n",
        "<li>20 for daily pay</li>\n",
        "<li>4 for weekly pay</li>\n",
        "<li>2 for fortnight pay</li>\n",
        "<li>1 for monthly pay</li>\n",
        "<li>1/12 for annual pay</li>\n",
        "</ul>\n",
        "The function will return the scaled salary after multiplying it by the scaling factor. In instances where the salary is not mentioned/ disclosed (or when no figure is given), the figure will return a NaN value (missing value) from Numpy.\n",
        "</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TMnhosayTEgX"
      },
      "outputs": [],
      "source": [
        "def salary_convert(b_string):\n",
        "\n",
        "  \"\"\"Scales (converts) a salary to a monthly rate\n",
        "\n",
        "  :param b_string: salary to be scaled\n",
        "  :return: a_string, scaled b_string\n",
        "\n",
        "  >>>salary_convert('R5000.00 - R11000.00 per fortnight')\n",
        "  10000.0\n",
        "  \"\"\"\n",
        "\n",
        "  if 'per hour' in b_string:          #hourly rate\n",
        "    a_string = b_string.strip('R').strip().split()[0].split('.')[0] \n",
        "    a_string = float(a_string.replace(',', '')) * 160\n",
        "\n",
        "  elif 'per day' in b_string:         #daily rate\n",
        "    a_string = b_string.strip('R').strip().split()[0].split('.')[0]\n",
        "    a_string = float(a_string.replace(',', '')) * 20\n",
        "\n",
        "  elif 'per week' in b_string:         #weekly rate\n",
        "    a_string = b_string.strip('R').split()[0].split('.')[0]\n",
        "    a_string = float(a_string.replace(',', '')) * 4\n",
        "\n",
        "  elif 'per fort' in b_string:         #fortnight rate\n",
        "    a_string = b_string.strip('R').strip().split()[0].split('.')[0]\n",
        "    a_string = float(a_string.replace(',', '')) * 2\n",
        "\n",
        "  elif 'per month' in b_string:        #monthly rate\n",
        "    a_string = b_string.strip('R').strip().split()[0].split('.')[0]\n",
        "    a_string = float(a_string.replace(',', ''))\n",
        "\n",
        "  elif 'per year' in b_string:       #yearly rate\n",
        "    a_string = b_string.strip('R').strip().split()[0].split('.')[0]\n",
        "    a_string = float(a_string.replace(',', '')) / 12\n",
        "\n",
        "  else:    #Undisclosed salaries\n",
        "    a_string = np.NaN\n",
        "\n",
        "  return a_string"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###***Get a record***\n",
        "\n",
        "<p align=justify> A job title, company, location, salary, posting date, expiry date and link will all be added as Key and value pairs to the individual job dictionary. Monthly salary will be calculated by calling the salary_convert function and added to the dictionary. The duration of the ad in days from day of posting to expiry date will be calculated by subtracting the posting date from the expiry date. Also calculated, are; the days left before the ad expires as the difference between the expiry date and the day of extraction (scraping). Both these will be added as individual key-value pairs to the dictionary. The function will return the dictionary\n",
        "</p>"
      ],
      "metadata": {
        "id": "WBbvvVW9mZEE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_record(a_card):\n",
        "  \"\"\"Builds a record for one job\n",
        "\n",
        "  :param a_card: a card to extract features from in order to build a record\n",
        "  :return: a dictionary of the features extracted \n",
        "  \"\"\"\n",
        "  \n",
        "  a_job = {}            #dictionary to hold individual job information\n",
        "        \n",
        "  prim_link = \"https://www.careerjunction.co.za\"\n",
        "  j_today = date.today()\n",
        "  j_title = a_card.find('h2')\n",
        "\n",
        "  a_job['Job Title'] = j_title.text.strip()\n",
        "  if a_card.find('h3') is not None:\n",
        "    a_job['Company'] = a_card.find('h3').text.strip()\n",
        "  \n",
        "  a_job['Location'] = a_card.find('li', class_=\"location\").text.strip()\n",
        "  a_job['Job Type'] = a_card.find('li', class_=\"position\").text.strip()\n",
        "\n",
        "  a_job['Salary'] = a_card.find('li', class_=\"salary\").text.strip()\n",
        "  a_job['Monthly Salary'] = salary_convert(a_job['Salary'])\n",
        "\n",
        "  a_job['Posting Date'] = a_card.find('li', class_=\"updated-time\").text\\\n",
        "  .strip('Posted').strip()\n",
        "  j_expiry = a_card.find('li', class_=\"expires\").text.split()[2]\n",
        "  try:\n",
        "    ini_exp = j_today + timedelta(days=int(j_expiry))\n",
        "  except ValueError:\n",
        "    ini_exp = j_today\n",
        "  a_job['Expiry Date'] = ini_exp.strftime('%d %b %Y')\n",
        "  j_duration = pd.to_datetime(a_job['Expiry Date'])-pd.to_datetime(a_job['Posting Date'])\n",
        "  j_toExpire = pd.to_datetime(a_job['Expiry Date'])-pd.to_datetime(j_today.strftime('%d %b %Y'))\n",
        "  a_job['Ad Duration (Days)'] = int(str(j_duration).split()[0])\n",
        "  a_job['Days before expiry'] = int(str(j_toExpire).split()[0])\n",
        "\n",
        "  a_job['Link'] = prim_link + j_title.find('a').get('href')\n",
        "\n",
        "  return a_job"
      ],
      "metadata": {
        "id": "QC1GgmvFmRdt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fwT4L4H-vai8"
      },
      "source": [
        "### ***Making the Soup***\n",
        "<p align=justify>\n",
        "The make_soup function will take job (string) and location (string, but has a default 0 value) parameters. The first step is to pass these parameters to the get_url function and create a url. This will be followed by adding a page number to the url and creating a request. The request content; response will be be passed through Beautiful Soup which will help us create the Soup variable that we will call pasta. Jobs will be set as the jobs container part of the pasta and individual job cards will be stored as j_cards. We will loop over the j_cards list, every job card will have its dictionary that will be added to the job collection list following each iteration. This process will continue until the needed information from each job on one page is collected, the process will continue for every page until the last page, which will be set to a maximum of 300 paged due to some shortcomings. make_soup will return job_collection, a list of dictionaries of every scraped job.\n",
        "</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vBYIV2kVXgc5"
      },
      "outputs": [],
      "source": [
        "def make_soup(job, location=0):\n",
        "  \n",
        "  \"\"\"Builds a soup and extracts the jobs from the website\n",
        "\n",
        "  :param job: job title to search for\n",
        "  :param location: the location to restrict the search to\n",
        "  :return: a list of dictionaries for every job that is scraped\n",
        "  \"\"\"\n",
        "\n",
        "  gen_url = get_url(job, location)    #defines the website that will be scraped\n",
        "  job_collection = []    #empty list to contain all the extracted job attributes \n",
        "\n",
        "  for i in range(1,300):\n",
        "    search_page = f\"&page={str(i)}\"          #this is the start page\n",
        "    url = gen_url + search_page             #add a page to the url\n",
        "    the_response = requests.get(url)  \n",
        "\n",
        "    pasta = BeautifulSoup(the_response.content, 'html.parser') #create the soup\n",
        "    jobs = pasta.find('div', class_=\"two-thirds\")          #location of the jobs on the page\n",
        "    try:\n",
        "      j_cards = jobs.find_all('div', class_=\"job-result\")    #collection of the jobs on the page\n",
        "    \n",
        "    #looping over the job cards list to extract features of each job\n",
        "      for j_card in j_cards:\n",
        "        a_record = get_record(j_card)  \n",
        "        job_collection.append(a_record) \n",
        "        #adds the single jobs dictionary to the jobs collection list, feature names are the keys\n",
        "    except AttributeError:\n",
        "      break    #this break the function if it has run out of pages to scrap\n",
        "\n",
        "  return job_collection    #returns jobs list\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z3b8WIqevfhk"
      },
      "source": [
        "### ***Creating a data frame and CSV file***\n",
        "\n",
        "<p align=justify>\n",
        "The last function, main_jobs, will take three parameters, job, location (with default value of zero), and the file_name; with defualt value 'your_file'. This function calls make_soup and passes the first two arguments. The resulting list of dictionaries will be converted to a pandas dataframe, then the columns will be rearranged for easier browsing following which, a CSV will be written from the resulting dataframe, it will take its name from the third parameter. The function also returns this dataframe. \n",
        "</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PNx6z5IUfyTB"
      },
      "outputs": [],
      "source": [
        "def make_df(job, location=0, file_name='your_file'):\n",
        "\n",
        "  \"\"\"Creates a datafrom from and a csv file\n",
        "\n",
        "  :params job: job title to search for\n",
        "  :params location: location to restrict the search to\n",
        "  :param file_name: the name the csv file will take\n",
        "  :return: returs a dataframe\n",
        "\n",
        "  \"\"\"\n",
        "    \n",
        "  cols_ord = ['Job Title', 'Location', 'Job Type', 'Company', 'Salary', \\\n",
        "              'Monthly Salary', 'Posting Date', 'Expiry Date', \\\n",
        "              'Ad Duration (Days)','Days before expiry', 'Link']\n",
        "\n",
        "  j_data = make_soup(job, location)  #defines a list of dictionaries\n",
        "  df = pd.DataFrame(j_data)          #coverts the list above to a dataframe\n",
        "  df= df[cols_ord]                   #reorders the columns\n",
        "  df.to_csv(file_name+'.csv')   #creates a jobs_data csv file from the dataframe\n",
        "\n",
        "  return df "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eFm7slOrvnwO"
      },
      "source": [
        "### ***Some tests***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6CSM2gCjZA2Z"
      },
      "outputs": [],
      "source": [
        "engineerWC = make_df('engineer', 'Western Cape', file_name='engineering jobs')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataZA = make_df('data', file_name='data jobs')"
      ],
      "metadata": {
        "id": "yPJTCGhIuXhz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "managerGP = make_df('manager', 'Gauteng', file_name='managerial jobs')"
      ],
      "metadata": {
        "id": "F6CJw_EvuTu9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "developerGP = make_df('developer', location='gauteng', file_name='halala')"
      ],
      "metadata": {
        "id": "PCyqehPgqSai"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZDxLfEs3nWe1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "318cc62c-99e6-407b-edfb-a56ddb52f344"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "481 0 22\n"
          ]
        }
      ],
      "source": [
        "print(dataZA['Ad Duration (Days)'].max(), dataZA['Ad Duration (Days)']\\\n",
        "      .min(), round(dataZA['Ad Duration (Days)'].mean()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7f3iiIYvwjn"
      },
      "source": [
        "### ***Attempt to wrapping inside a class***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wUrHMP1Xqtvm"
      },
      "outputs": [],
      "source": [
        "class JobScraping():\n",
        "\n",
        "  \"\"\"Scrap the website for jobs\n",
        "\n",
        "  :param job: job title to search for\n",
        "  :param locs: location to restrict the search to\n",
        "  :param file_name: name that the csv file will take\n",
        "\n",
        "  :ivar locations: the locations encoding\n",
        "  :ivar the_url: the url\n",
        "  :ivar the_soup: the soup thats created for extracting the job features\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, job, locs=0, file_name='your_file'):\n",
        "    self.locations = {'Gauteng':'2747', 'Western Cape':'16149', 'KwaZulu-Natal':'13131',\\\n",
        "                      'Eastern Cape':'2', 'Free State':'1782', 'Mpumalanga':'14867',\\\n",
        "                      'Limpopo':'14594', 'North West':'15372', 'Northern Cape':'15837',\\\n",
        "                      'International':'1000001', 'Working From Home':'100000'}\n",
        "    # above is the locations dictionary that will encode the location from a \n",
        "    # normal word search to an 'int' search\n",
        "\n",
        "    self.job = job\n",
        "    self.locs = locs\n",
        "    self.file_name = file_name\n",
        "    self.the_url = self.__get_url\n",
        "    self.the_soup = self.__soup_set\n",
        "\n",
        "  def __get_url(self):\n",
        "    # When no location is specified, default location = 0, i.e. the search will \n",
        "    # not have any location restrictions\n",
        " \n",
        "    error_message = 'No access to location, please revise'\n",
        "    \n",
        "    try:\n",
        "      if self.locs != 0:                    #encode the location\n",
        "        self.locs = self.locations[self.locs.title()]\n",
        "      gen_url = \"https://www.careerjunction.co.za/jobs/results?keywords={}&autosuggestEndpoint=%2Fautosuggest&location={}&category=&btnSubmit=+\"\n",
        "      search_url = gen_url.format(self.job, self.locs)\n",
        "      \n",
        "      return search_url\n",
        "    \n",
        "    except KeyError:\n",
        "      return error_message\n",
        "\n",
        "  def sal_set(self, sal):\n",
        "\n",
        "      if 'per hour' in sal:    #hourly rate\n",
        "        a_string = sal.strip('R').strip().split()[0].split('.')[0] \n",
        "        a_string = float(a_string.replace(',', '')) * 160\n",
        "\n",
        "      elif 'per day' in sal:         #daily rate\n",
        "        a_string = sal.strip('R').strip().split()[0].split('.')[0]\n",
        "        a_string = float(a_string.replace(',', '')) * 20\n",
        "\n",
        "      elif 'per week' in sal:         #weekly rate\n",
        "        a_string = sal.strip('R').split()[0].split('.')[0]\n",
        "        a_string = float(a_string.replace(',', '')) * 4\n",
        "\n",
        "      elif 'per fort' in sal:         #fortnight rate\n",
        "        a_string = sal.strip('R').strip().split()[0].split('.')[0]\n",
        "        a_string = float(a_string.replace(',', '')) * 2\n",
        "\n",
        "      elif 'per month' in sal:        #monthly rate\n",
        "        a_string = sal.strip('R').strip().split()[0].split('.')[0]\n",
        "        a_string = float(a_string.replace(',', ''))\n",
        "\n",
        "      elif 'per year' in sal:       #yearly rate\n",
        "        a_string = sal.strip('R').strip().split()[0].split('.')[0]\n",
        "        a_string = float(a_string.replace(',', '')) / 12\n",
        "\n",
        "      else:    #Undisclosed salaries\n",
        "        a_string = np.NaN\n",
        "\n",
        "      return a_string\n",
        "\n",
        "  def __soup_set(self):\n",
        "  \n",
        "    gen_url = self.__get_url()    #defines the website that will be scraped\n",
        "    job_collection = []    #empty list to contain all the extracted job attributes \n",
        "\n",
        "    for i in range(1,200):\n",
        "      search_page = f\"&page={str(i)}\"          #this is the start page\n",
        "      url = gen_url + search_page             #add a page to the url\n",
        "      the_response = requests.get(url)  \n",
        "\n",
        "      pasta = BeautifulSoup(the_response.content, 'html.parser') #create the soup\n",
        "      jobs = pasta.find('div', class_=\"two-thirds\")          #location of the jobs on the page\n",
        "      try:\n",
        "        j_cards = jobs.find_all('div', class_=\"job-result\")    #collection of the jobs on the page\n",
        "      \n",
        "        for j_card in j_cards:  #looping over the job cards list to extract features of each job\n",
        "          a_job = {}            #dictionary to hold individual job information\n",
        "          \n",
        "          prim_link = \"https://www.careerjunction.co.za\"\n",
        "          j_today = date.today()\n",
        "          j_title = j_card.find('h2')\n",
        "          \n",
        "          a_job['Job Title'] = j_title.text.strip()\n",
        "          if j_card.find('h3') is not None:\n",
        "            a_job['Company'] = j_card.find('h3').text.strip()\n",
        "          a_job['Location'] = j_card.find('li', class_=\"location\").text.strip()\n",
        "          a_job['Job Type'] = j_card.find('li', class_=\"position\").text.strip()\n",
        "\n",
        "          a_job['Salary'] = j_card.find('li', class_=\"salary\").text.strip()\n",
        "          a_job['Monthly Salary'] = self.sal_set(a_job['Salary'])\n",
        "\n",
        "          a_job['Posting Date'] = j_card.find('li', class_=\"updated-time\").text\\\n",
        "          .strip('Posted').strip()\n",
        "\n",
        "          j_expiry = j_card.find('li', class_=\"expires\").text.split()[2]\n",
        "          try:\n",
        "            ini_exp = j_today + timedelta(days=int(j_expiry))\n",
        "          except ValueError:\n",
        "            ini_exp = j_today\n",
        "          a_job['Expiry Date'] = ini_exp.strftime('%d %b %Y')\n",
        "          j_duration = pd.to_datetime(a_job['Expiry Date'])-pd.to_datetime(a_job['Posting Date'])\n",
        "          j_toExpire = pd.to_datetime(a_job['Expiry Date'])-pd.to_datetime(j_today.strftime('%d %b %Y'))\n",
        "          a_job['Ad Duration (Days)'] = int(str(j_duration).split()[0])\n",
        "          a_job['Days before expiry'] = int(str(j_toExpire).split()[0])\n",
        "\n",
        "          a_job['Link'] = prim_link + j_title.find('a').get('href')\n",
        "\n",
        "          job_collection.append(a_job) \n",
        "          #adds the single jobs dictionary to the jobs collection list, feature names are the keys\n",
        "      except AttributeError:\n",
        "        break    #this break the function if it has run out of pages to scrap\n",
        "\n",
        "    return job_collection    #returns jobs list\n",
        "\n",
        "\n",
        "  def main_run(self):\n",
        "      \n",
        "      cols_ord = ['Job Title', 'Location', 'Job Type', 'Company', 'Salary', \\\n",
        "                  'Monthly Salary', 'Posting Date', 'Expiry Date', \\\n",
        "                  'Ad Duration (Days)','Days before expiry', 'Link']\n",
        "\n",
        "      j_data = self.__soup_set()           #defines a list of dictionaries\n",
        "      df = pd.DataFrame(j_data)          #coverts the list above to a dataframe\n",
        "      df= df[cols_ord]                   #reorders the columns\n",
        "      df.to_csv(self.file_name+'.csv')   #creates a jobs_data csv file from the dataframe\n",
        "\n",
        "      return df "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "manager_jobs = JobScraping('manager', file_name='managerjobs3')\n",
        "mngr_file = manager_jobs.main_run()\n"
      ],
      "metadata": {
        "id": "TJCUSW1PdHXY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a391869-a8fd-4be5-8a48-8bfce0fadb4b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<bound method JobScraping.__get_url of <__main__.JobScraping object at 0x7f4e6cfef640>>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mngr_file"
      ],
      "metadata": {
        "id": "2wwq-n0d5Yrn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mngr_file[mngr_file['Days before expiry']<10]"
      ],
      "metadata": {
        "id": "uHjedmvy5VI7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mngr_file[mngr_file['Days before expiry'] == 20]"
      ],
      "metadata": {
        "id": "_AFuHon35V6B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(manager_jobs.the_url)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W0B5_-lFz4gC",
        "outputId": "0aee085a-fcea-494c-be28-17525a74afe2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<bound method JobScraping.__get_url of <__main__.JobScraping object at 0x7f4e75c1e070>>\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "H8Lsgwq6HBDB",
        "qRw6Wd8rkmol",
        "AZTFv6xDrXq-",
        "WBbvvVW9mZEE",
        "Z3b8WIqevfhk",
        "eFm7slOrvnwO"
      ],
      "provenance": [],
      "authorship_tag": "ABX9TyNiX04VHdNDsa6ihCQHFyar",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}