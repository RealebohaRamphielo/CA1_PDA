{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RealebohaRamphielo/CA1_PDA/blob/main/Realeboha_Ramphielo(10622234)_CA2_Programming_for_Data_Analysis_B9AI108_Dec22.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wcyWEToIFQIF"
      },
      "source": [
        "<h1><b>Realeboha Raymond Ramphielo (10622234)</b></h1>\n",
        "\n",
        "<h3><i> B9AI108 Programming For Data Analysis B9AI108_2223_TMD1S </h3>\n",
        "<h3> CA 2 </i></h3>\n",
        "\n",
        "<a>[link text](https://colab.research.google.com/drive/1XIZa4PnihdmvbuFt9s89LH5AF3aYZIFr?usp=sharing)</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<ul type=None>\n",
        "<li><a href=\"https://github.com/RealebohaRamphielo/CA1_PDA\"> GitHub link </a></li>\n",
        "<li><a href=\"https://colab.research.google.com/drive/1XIZa4PnihdmvbuFt9s89LH5AF3aYZIFr?usp=sharing\"> Colab Link </a></li>\n",
        "</ul>\n",
        "<p align=justify>Please remmeber to open in jupyter as colab might bring some issues when getting the url. The project is hosted in colab, so regular updates are on colab, a copy was saved to GitHub for backup purposes, no real work was done on GitHub</p>"
      ],
      "metadata": {
        "id": "Pu8t0o2xGI8W"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CzRU3vnhrTPc"
      },
      "source": [
        "## **Intoduction**\n",
        "<p align=justify> Job scraping is the application of web scraping to employment. Users can easily collect employment information and build a meaningful job database using this automated online data extraction technique. There are several websites to fetch job data, these range from big job boards like <i>Irishjobs.ie</i> and the likes, right down to individual companiesâ€™ career portals. Jobs scraping involves gathering information regarding jobs and downloading it into structured and usable formats, like CSV, that can be used for analysis (Gangadhara Reddy and Viswanath, 2022). An obvious advantage to job scraping is probably data relevancy and timeliness. I will capture the HTML text using the standard Requests and Beautiful Soup configurations, and then convert it to a Beautiful Soup object to simply parse through it and retrieve the data points. I plan to scrap the Career Junction website (South African). From the website, I will scrap the Job title, copany, location, salary, day of posting, day of expiry, and a link to the job. </p>  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H8Lsgwq6HBDB"
      },
      "source": [
        "## Libarary intallations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eb1fYUPb_eB7",
        "outputId": "50ebff15-b1a8-4c0b-9791-29fef6bdf6af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (2.23.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests) (2022.9.24)\n"
          ]
        }
      ],
      "source": [
        "pip install requests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WaNrME8M_irW",
        "outputId": "832281ac-c8e1-408f-ff91-8ba7716b4c61"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: bs4 in /usr/local/lib/python3.8/dist-packages (0.0.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.8/dist-packages (from bs4) (4.6.3)\n"
          ]
        }
      ],
      "source": [
        "pip install bs4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "2ehHDOAutHXw"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import json\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "from datetime import datetime, date, timedelta \n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pdHx1dJR47i8"
      },
      "source": [
        "## **Steps**\n",
        "<ol>\n",
        "<li>Getting the url</li>\n",
        "<li>Scaling the salary</li>\n",
        "<li>Maing the soup</li>\n",
        "<li>Creating the dataframe and csv</li>\n",
        "</ol>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qRw6Wd8rkmol"
      },
      "source": [
        "### ***Getting the url*** \n",
        "<p align=justify> The get_url function will accept two string parameters; job and location, location is set at default value of 0 which translates to no location. Within this function is a dictionary variable, locations. Locations will convert/ encode the second parameter into a usable location code by pointing at the location's value in the locations deictionary. After encoding the location, the next step is to 'plug' the desired job title and encoded location into the ural and create a url that will be returned by the function. In a case where the location is not defined, a KeyValueError will be raised by the function and an error message will be returned. </p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "xpm-xKLhr1Qp"
      },
      "outputs": [],
      "source": [
        "def get_url(job, location = 0):\n",
        "  \n",
        "  '''This function sets up the search url from the user defines job and location,\n",
        "    default location will be 0'''\n",
        "\n",
        "  #When no location is specified, default location = 0, i.e. the search will not\n",
        "  # have any location restrictions\n",
        "\n",
        "  locations = {'Gauteng':'2747', 'Western Cape':'16149', 'KwaZulu-Natal':'13131',\\\n",
        "              'Eastern Cape':'2', 'Free State':'1782', 'Mpumalanga':'14867',\\\n",
        "              'Limpopo':'14594', 'North West':'15372', 'Northern Cape':'15837',\\\n",
        "              'International':'1000001', 'Working From Home':'100000'}\n",
        "\n",
        "  #above is the locations dictionary that will encode the location from a normal\n",
        "  #word search to an 'int' search\n",
        "  \n",
        "  error_message = 'No access to location, please revise'\n",
        "  \n",
        "  try:\n",
        "    if location != 0:                    #encode the location\n",
        "      location = locations[location]\n",
        "    gen_url = \"https://www.careerjunction.co.za/jobs/results?keywords={}&autosuggestEndpoint=%2Fautosuggest&location={}&category=&btnSubmit=+\"\n",
        "    search_url = gen_url.format(job, location)\n",
        "    \n",
        "    return search_url\n",
        "  \n",
        "  except KeyError:\n",
        "    return error_message"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AZTFv6xDrXq-"
      },
      "source": [
        "###***Scaling the salary***\n",
        "<p align=justify> The salary_convert funtion will take one string parameter. The purpose of this function is to scale the salary to monthly rates. It will first strip the string off the 'R' which denotes the ZAR (South AFrican Rand), then strip off any white split the string using white space. Following the split, we will take the first element (element at index 0) and follow this by splitting it using the period (dot) character and assign to a_string variable the first element from the resulting split. Some figures are comma separated, thus the second step will be to get remove any commas from a_string and covert the variable to a float and multiply it by a scaling factor to calculate a monthly salary, the scaling factors are as follows:\n",
        "<ul>\n",
        "<li>160 for hourly pay</li>\n",
        "<li>20 for daily pay</li>\n",
        "<li>4 for weekly pay</li>\n",
        "<li>2 for fortnight pay</li>\n",
        "<li>1 for monthly pay</li>\n",
        "<li>1/12 for annual pay</li>\n",
        "</ul>\n",
        "The function will return the scaled salary after multiplying it by the scaling factor. In instances where the salary is not mentioned/ disclosed (or when no figure is given), the figure will return a NaN value (missing value) from Numpy.\n",
        "</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "TMnhosayTEgX"
      },
      "outputs": [],
      "source": [
        "def salary_convert(b_string):\n",
        "\n",
        "  ''' The function will convert help extract salaries and scales them to a \n",
        "    monthly value'''\n",
        "\n",
        "  if 'per hour' in b_string:          #hourly rate\n",
        "    a_string = b_string.strip('R').strip().split()[0].split('.')[0] \n",
        "    a_string = float(a_string.replace(',', '')) * 160\n",
        "\n",
        "  elif 'per day' in b_string:         #daily rate\n",
        "    a_string = b_string.strip('R').strip().split()[0].split('.')[0]\n",
        "    a_string = float(a_string.replace(',', '')) * 20\n",
        "\n",
        "  elif 'per week' in b_string:         #weekly rate\n",
        "    a_string = b_string.strip('R').split()[0].split('.')[0]\n",
        "    a_string = float(a_string.replace(',', '')) * 4\n",
        "\n",
        "  elif 'per fort' in b_string:         #fortnight rate\n",
        "    a_string = b_string.strip('R').strip().split()[0].split('.')[0]\n",
        "    a_string = float(a_string.replace(',', '')) * 2\n",
        "\n",
        "  elif 'per month' in b_string:        #monthly rate\n",
        "    a_string = b_string.strip('R').strip().split()[0].split('.')[0]\n",
        "    a_string = float(a_string.replace(',', ''))\n",
        "\n",
        "  elif 'per year' in b_string:       #yearly rate\n",
        "    a_string = b_string.strip('R').strip().split()[0].split('.')[0]\n",
        "    a_string = float(a_string.replace(',', '')) / 12\n",
        "\n",
        "  else:    #Undisclosed salaries\n",
        "    a_string = np.NaN\n",
        "\n",
        "  return a_string"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fwT4L4H-vai8"
      },
      "source": [
        "### ***Making the Soup***\n",
        "<p align=justify>\n",
        "The make_soup function will take job (string) and location (string, but has a default 0 value) parameters. The first step is to pass these parameters to the get_url function and create the url. This will be followed by adding the page number to the url and creating a request. The request content; response response will be be passed through Beautiful Soup whuch will help us create the Soup variable pasta. Jobs will be set as the jobs container part of the pasta and individual job cards will be stored as j_cards. We will the loop over the j_cards list, every job card will have its dictionary that will later be added to an empty list of job collection following each iteration. A job title, company, location, salary, posting date, expiry date and link will all be added as Key and value pairs to the individaul job dictionary. Monthly salary will be calculated by calling the salary_convert function and added to the dictionary. The duration of the ad in days from day of posting to expiry date will be calculated within the for loop by subtracting the posting date from the expiry date. Also calculated within the for loop, the days left before the ad expires will be the difference between the expiry date and the day of exctraction  (scraping). Both these will be added as individual key-value pairs to the dictionary. This process will continue until the needed information from each jobs on one page is collected, the process will the continue for every page until the last page, which is usually a maximum of 30 for this website. make_soup will return job_collection, a list of dictionaries of every scraped job.\n",
        "</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "vBYIV2kVXgc5"
      },
      "outputs": [],
      "source": [
        "def make_soup(job, location=0):\n",
        "  \n",
        "  '''The function creates a response from the created by get_url, then scraps \n",
        "    the actual data from the website'''\n",
        "\n",
        "  gen_url = get_url(job, location)    #defines the website that will be scraped\n",
        "  job_collection = []    #empty list to contain all the extracted job attributes \n",
        "\n",
        "  for i in range(1,30):\n",
        "    search_page = f\"&page={str(i)}\"          #this is the start page\n",
        "    url = gen_url + search_page             #add a page to the url\n",
        "    the_response = requests.get(url)  \n",
        "\n",
        "    pasta = BeautifulSoup(the_response.content, 'html.parser') #create the soup\n",
        "    jobs = pasta.find('div', class_=\"two-thirds\")          #location of the jobs on the page\n",
        "    try:\n",
        "      j_cards = jobs.find_all('div', class_=\"job-result\")    #collection of the jobs on the page\n",
        "    \n",
        "      for j_card in j_cards:  #looping over the job cards list to extract features of each job\n",
        "        a_job = {}            #dictionary to hold individual job information\n",
        "        \n",
        "        prim_link = \"https://www.careerjunction.co.za\"\n",
        "        j_today = date.today()\n",
        "        j_title = j_card.find('h2')\n",
        "        \n",
        "        a_job['Job Title'] = j_title.text.strip()\n",
        "        if j_card.find('h3') is not None:\n",
        "          a_job['Company'] = j_card.find('h3').text.strip()\n",
        "        a_job['Location'] = j_card.find('li', class_=\"location\").text.strip()\n",
        "        a_job['Job Type'] = j_card.find('li', class_=\"position\").text.strip()\n",
        "\n",
        "        a_job['Salary'] = j_card.find('li', class_=\"salary\").text.strip()\n",
        "        a_job['Monthly Salary'] = salary_convert(a_job['Salary'])\n",
        "\n",
        "        a_job['Posting Date'] = j_card.find('li', class_=\"updated-time\").text\\\n",
        "        .strip('Posted').strip()\n",
        "\n",
        "        j_expiry = j_card.find('li', class_=\"expires\").text.split()[2]\n",
        "        try:\n",
        "          ini_exp = j_today + timedelta(days=int(j_expiry))\n",
        "        except ValueError:\n",
        "          ini_exp = j_today\n",
        "        a_job['Expiry Date'] = ini_exp.strftime('%d %b %Y')\n",
        "        j_duration = pd.to_datetime(a_job['Expiry Date'])-pd.to_datetime(a_job['Posting Date'])\n",
        "        j_toExpire = pd.to_datetime(a_job['Expiry Date'])-pd.to_datetime(j_today.strftime('%d %b %Y'))\n",
        "        a_job['Ad Duration (Days)'] = int(str(j_duration).split()[0])\n",
        "        a_job['Days before expiry'] = int(str(j_toExpire).split()[0])\n",
        "\n",
        "        a_job['Link'] = prim_link + j_title.find('a').get('href')\n",
        "\n",
        "        job_collection.append(a_job) \n",
        "        #adds the single jobs dictionary to the jobs collection list, feature names are the keys\n",
        "    except AttributeError:\n",
        "      break    #this break the function if it has run out of pages to scrap\n",
        "\n",
        "  return job_collection    #returns jobs list\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z3b8WIqevfhk"
      },
      "source": [
        "### ***Creating a data frame and CSV file***\n",
        "\n",
        "<p align=justify>\n",
        "The last function, main_jobs, will take three parameters, job, location (has a default value of zero), and the file_name; with defualt value 'your_file' . This function calls make_soup and passes the first two arguments. The resulting list of dictionaries will be converted to a pandas dataframe, then the columns will be rearranged for easier browsing following which, a CSV will be written from the resulting dataframe, it will take its name from the third parameter. The function also returns this dataframe. \n",
        "</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "PNx6z5IUfyTB"
      },
      "outputs": [],
      "source": [
        "def main_jobs(job, location=0, file_name='your_file'):\n",
        "\n",
        "  '''Runs make_soup functions then creates a pandas dataframe that will be\n",
        "    converted into a csv file'''\n",
        "    \n",
        "  cols_ord = ['Job Title', 'Location', 'Job Type', 'Company', 'Salary', \\\n",
        "              'Monthly Salary', 'Posting Date', 'Expiry Date', \\\n",
        "              'Ad Duration (Days)','Days before expiry', 'Link']\n",
        "\n",
        "  j_data = make_soup(job, location)  #defines a list of dictionaries\n",
        "  df = pd.DataFrame(j_data)          #coverts the list above to a dataframe\n",
        "  df= df[cols_ord]                   #reorders the columns\n",
        "  df.to_csv(file_name+'.csv')   #creates a jobs_data csv file from the dataframe\n",
        "\n",
        "  return df "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eFm7slOrvnwO"
      },
      "source": [
        "### ***Some tests***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "6CSM2gCjZA2Z"
      },
      "outputs": [],
      "source": [
        "some_some = main_jobs('engineer', 'Western Cape', file_name='engineering jobs')\n",
        "some_some2 = main_jobs('data', file_name='data jobs')\n",
        "some_some3 = main_jobs('manager', 'Gauteng', file_name='managerial jobs')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZDxLfEs3nWe1"
      },
      "outputs": [],
      "source": [
        "print(some_some['Ad Duration (Days)'].max(), some_some['Ad Duration (Days)']\\\n",
        "      .min(), round(some_some['Ad Duration (Days)'].mean()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RCXX65vKLvr0"
      },
      "outputs": [],
      "source": [
        "# #from numpy.lib.shape_base import dsplit\n",
        "# li = []\n",
        "# for i in range(1,3):\n",
        "#   gen_url3 = \"https://www.careerjunction.co.za/jobs/results?keywords=data&autosuggestEndpoint=%2fautosuggest&location=2747&category=&btnSubmit=+\"\n",
        "#   #url1 = get_url('data', 'Limpopo')\n",
        "#   cococ = f\"&page={str(i)}\"\n",
        "#   gen_url = gen_url3 + cococ\n",
        "#   response = requests.get(gen_url)\n",
        "#   resp=response.content\n",
        "#   #print(resp, resp.reason)\n",
        "#   #print(gen_url)\n",
        "#   #print(url1)\n",
        "#   #resp.status_code\n",
        "#   pasta = BeautifulSoup(resp, 'html.parser')\n",
        "#   jobs = pasta.find('div', class_=\"two-thirds\")\n",
        "#   j_cards = jobs.find_all('div', class_=\"job-result\")\n",
        "#   #j_cards.pop(5)\n",
        "#   print(len(j_cards))\n",
        "#   #j_cards\n",
        "#   #li = []\n",
        "#   for j_card in j_cards:\n",
        "#     j_today = date.today()\n",
        "#     a_job = {}\n",
        "#     titleofjob = j_card.find('h2')\n",
        "#     a_job['Job Title'] = (titleofjob.text.strip())\n",
        "#   #  a_job['Company'] = (j_card.find('h3').text.strip())\n",
        "#     a_job['Location'] = (j_card.find('li', class_=\"location\").text.strip())\n",
        "#     a_job['Job Type'] = (j_card.find('li', class_=\"position\").text.strip())\n",
        "#     a_job['Salary'] = (j_card.find('li', class_=\"salary\").text.strip())\n",
        "#     a_job['Monthly Salary'] = salary_convert(a_job['Salary'])\n",
        "    \n",
        "#     a_job['Posting date'] = (j_card.find('li', class_=\"updated-time\").text.strip('Posted').strip())\n",
        "#     j_expiry = j_card.find('li', class_=\"expires\").text.split()[2]\n",
        "#     if j_expiry != 'today':\n",
        "#       ini_exp = j_today + timedelta(days=int(j_expiry))\n",
        "#       a_job['Expiry date'] = ini_exp.strftime('%d %b %Y')\n",
        "#     else:\n",
        "#       a_job['Expiry date'] = j_today.strftime('%d %b %Y')\n",
        "#     #a_job['Job URL'] = (j_card.find('h2').text.strip())\n",
        "#     duration1 = pd.to_datetime(a_job['Expiry date']) - pd.to_datetime(a_job['Posting date'])\n",
        "#     duration2 = pd.to_datetime(a_job['Expiry date']) - pd.to_datetime(j_today.strftime('%d %b %Y'))\n",
        "#     #a_job['j_title'] = (j_card.find('h2').text.strip())\n",
        "#     a_job['sahdis'] = int(str(duration1).split()[0])\n",
        "#     a_job['sahdis2'] = int(str(duration2).split()[0])\n",
        "#     a_job['Link']  = 'https://www.careerjunction.co.za'+titleofjob.find('a').get('href')\n",
        "\n",
        "#     li.append(a_job)\n",
        "\n",
        "#   #two-way step in accessing an embedded feature, find outer tag and assign it\n",
        "#   #then use asigned_varaible.attrs['some_tag'] to locate the embedded feature\n",
        "#   #eg -> var = j_card.find('h2') followed by feature = var.attrs['a'] \n",
        "\n",
        "#   #convert final list of dictionaries to a pandas dataframe then use to_csv after\n",
        "#   #df = pd.DataFrame(jobs) followed by df.to_csv('file_name.csv')\n",
        "\n",
        "#   #len(li)\n",
        "#   #li[3]\n",
        "# #li\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eP0wDLB7Hb1H"
      },
      "outputs": [],
      "source": [
        "gen_url100 = \"https://www.careerjunction.co.za/jobs/results?keywords=data&autosuggestEndpoint=%2fautosuggest&location=2747&category=&btnSubmit=+&page=2898\"\n",
        "resp100 = requests.get(gen_url100)\n",
        "print(resp100, resp100.reason) \n",
        "print(resp100.status_code)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9_M8qSBYQ2Mt"
      },
      "outputs": [],
      "source": [
        "# li = []\n",
        "# for j_card in j_cards:\n",
        "#   j_today = date.today()\n",
        "#   a_job = {}\n",
        "#   a_job['Job Title'] = (j_card.find('h2').text.strip())\n",
        "#   a_job['Company'] = (j_card.find('h3').text.strip())\n",
        "#   a_job['Location'] = (j_card.find('li', class_=\"location\").text.strip())\n",
        "#   a_job['Job Type'] = (j_card.find('li', class_=\"position\").text.strip())\n",
        "#   a_job['Salary'] = (j_card.find('li', class_=\"salary\").text.strip('R').strip())\n",
        "#   a_job['Posting date'] = (j_card.find('li', class_=\"updated-time\").text.strip('Posted').strip())\n",
        "#   j_expiry = j_card.find('li', class_=\"expires\").text.split()[2]\n",
        "#   if j_expiry != 'today':\n",
        "#     ini_exp = j_today + timedelta(days=int(j_expiry))\n",
        "#     a_job['Expiry date'] = ini_exp.strftime('%d %b %Y')\n",
        "#   else:\n",
        "#     a_job['Expiry date'] = j_today.strftime('%d %b %Y')\n",
        "#   a_job['Duration'] = pd.to_datetime(a_job['Expiry date']) - pd.to_datetime(a_job['Posting date'])\n",
        "#   #a_job['Job URL'] = (j_card.find('h2').text.strip())\n",
        "#   #a_job['j_title'] = (j_card.find('h2').text.strip())\n",
        "\n",
        "#   li.append(a_job)\n",
        "\n",
        "# #two-way step in accessing an embedded feature, find outer tag and assign it\n",
        "# #then use asigned_varaible.attrs['some_tag'] to locate the embedded feature\n",
        "# #eg -> var = j_card.find('h2') followed by feature = var.attrs['a'] \n",
        "\n",
        "# #convert final list of dictionaries to a pandas dataframe then use to_csv after\n",
        "# #df = pd.DataFrame(jobs) followed by df.to_csv('file_name.csv')\n",
        "\n",
        "# len(li)\n",
        "# li[3]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7f3iiIYvwjn"
      },
      "source": [
        "### Attempting wrapping inside a class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "wUrHMP1Xqtvm"
      },
      "outputs": [],
      "source": [
        "class JobScraping():\n",
        "  def __init__(self, job, locs=0, file_name='your_file'):\n",
        "    self.locations = {'Gauteng':'2747', 'Western Cape':'16149', 'KwaZulu-Natal':'13131',\\\n",
        "                      'Eastern Cape':'2', 'Free State':'1782', 'Mpumalanga':'14867',\\\n",
        "                      'Limpopo':'14594', 'North West':'15372', 'Northern Cape':'15837',\\\n",
        "                      'International':'1000001', 'Working From Home':'100000'}\n",
        "    # above is the locations dictionary that will encode the location from a \n",
        "    # normal word search to an 'int' search\n",
        "    self.job = job\n",
        "    self.locs = locs\n",
        "    self.file_name = file_name\n",
        "    self.the_url = self._get_url\n",
        "    self.the_soup = self._soup_set\n",
        "    self.the_file = self._main_run\n",
        "\n",
        "  def _get_url(self):\n",
        "    '''This method sets up the search url from the user defines job and \n",
        "    location, default location will be 0'''\n",
        "\n",
        "    # When no location is specified, default location = 0, i.e. the search will \n",
        "    # not have any location restrictions\n",
        " \n",
        "    error_message = 'No access to location, please revise'\n",
        "    \n",
        "    try:\n",
        "      if self.locs != 0:                    #encode the location\n",
        "        self.locs = self.locations[self.locs]\n",
        "      gen_url = \"https://www.careerjunction.co.za/jobs/results?keywords={}&autosuggestEndpoint=%2Fautosuggest&location={}&category=&btnSubmit=+\"\n",
        "      search_url = gen_url.format(self.job, self.locs)\n",
        "      \n",
        "      return search_url\n",
        "    \n",
        "    except KeyError:\n",
        "      return error_message\n",
        "\n",
        "  def sal_set(self, sal):\n",
        "      ''' The method will convert help extract salaries and scales them to a \n",
        "      monthly value'''\n",
        "\n",
        "      if 'per hour' in sal:    #hourly rate\n",
        "        a_string = sal.strip('R').strip().split()[0].split('.')[0] \n",
        "        a_string = float(a_string.replace(',', '')) * 160\n",
        "\n",
        "      elif 'per day' in sal:         #daily rate\n",
        "        a_string = sal.strip('R').strip().split()[0].split('.')[0]\n",
        "        a_string = float(a_string.replace(',', '')) * 20\n",
        "\n",
        "      elif 'per week' in sal:         #weekly rate\n",
        "        a_string = sal.strip('R').split()[0].split('.')[0]\n",
        "        a_string = float(a_string.replace(',', '')) * 4\n",
        "\n",
        "      elif 'per fort' in sal:         #fortnight rate\n",
        "        a_string = sal.strip('R').strip().split()[0].split('.')[0]\n",
        "        a_string = float(a_string.replace(',', '')) * 2\n",
        "\n",
        "      elif 'per month' in sal:        #monthly rate\n",
        "        a_string = sal.strip('R').strip().split()[0].split('.')[0]\n",
        "        a_string = float(a_string.replace(',', ''))\n",
        "\n",
        "      elif 'per year' in sal:       #yearly rate\n",
        "        a_string = sal.strip('R').strip().split()[0].split('.')[0]\n",
        "        a_string = float(a_string.replace(',', '')) / 12\n",
        "\n",
        "      else:    #Undisclosed salaries\n",
        "        a_string = np.NaN\n",
        "\n",
        "      return a_string\n",
        "\n",
        "  def _soup_set(self):\n",
        "    '''The method creates a response from the created by get_url, then scraps \n",
        "    the actual data from the website'''\n",
        "\n",
        "    gen_url = self.get_url(self.job, self.locs)    #defines the website that will be scraped\n",
        "    job_collection = []    #empty list to contain all the extracted job attributes \n",
        "\n",
        "    for i in range(1,200):\n",
        "      search_page = f\"&page={str(i)}\"          #this is the start page\n",
        "      url = gen_url + search_page             #add a page to the url\n",
        "      the_response = requests.get(url)  \n",
        "\n",
        "      pasta = BeautifulSoup(the_response.content, 'html.parser') #create the soup\n",
        "      jobs = pasta.find('div', class_=\"two-thirds\")          #location of the jobs on the page\n",
        "      try:\n",
        "        j_cards = jobs.find_all('div', class_=\"job-result\")    #collection of the jobs on the page\n",
        "      \n",
        "        for j_card in j_cards:  #looping over the job cards list to extract features of each job\n",
        "          a_job = {}            #dictionary to hold individual job information\n",
        "          \n",
        "          prim_link = \"https://www.careerjunction.co.za\"\n",
        "          j_today = date.today()\n",
        "          j_title = j_card.find('h2')\n",
        "          \n",
        "          a_job['Job Title'] = j_title.text.strip()\n",
        "          if j_card.find('h3') is not None:\n",
        "            a_job['Company'] = j_card.find('h3').text.strip()\n",
        "          a_job['Location'] = j_card.find('li', class_=\"location\").text.strip()\n",
        "          a_job['Job Type'] = j_card.find('li', class_=\"position\").text.strip()\n",
        "\n",
        "          a_job['Salary'] = j_card.find('li', class_=\"salary\").text.strip()\n",
        "          a_job['Monthly Salary'] = self.sal_set(a_job['Salary'])\n",
        "\n",
        "          a_job['Posting Date'] = j_card.find('li', class_=\"updated-time\").text\\\n",
        "          .strip('Posted').strip()\n",
        "\n",
        "          j_expiry = j_card.find('li', class_=\"expires\").text.split()[2]\n",
        "          try:\n",
        "            ini_exp = j_today + timedelta(days=int(j_expiry))\n",
        "          except ValueError:\n",
        "            ini_exp = j_today\n",
        "          a_job['Expiry Date'] = ini_exp.strftime('%d %b %Y')\n",
        "          j_duration = pd.to_datetime(a_job['Expiry Date'])-pd.to_datetime(a_job['Posting Date'])\n",
        "          j_toExpire = pd.to_datetime(a_job['Expiry Date'])-pd.to_datetime(j_today.strftime('%d %b %Y'))\n",
        "          a_job['Ad Duration (Days)'] = int(str(j_duration).split()[0])\n",
        "          a_job['Days before expiry'] = int(str(j_toExpire).split()[0])\n",
        "\n",
        "          a_job['Link'] = prim_link + j_title.find('a').get('href')\n",
        "\n",
        "          job_collection.append(a_job) \n",
        "          #adds the single jobs dictionary to the jobs collection list, feature names are the keys\n",
        "      except AttributeError:\n",
        "        break    #this break the function if it has run out of pages to scrap\n",
        "\n",
        "    return job_collection    #returns jobs list\n",
        "\n",
        "\n",
        "  def _main_run(self):\n",
        "      '''Runs make_soup functions then creates a pandas dataframe that will be\n",
        "      converted into a csv file'''\n",
        "      \n",
        "      cols_ord = ['Job Title', 'Location', 'Job Type', 'Company', 'Salary', \\\n",
        "                  'Monthly Salary', 'Posting Date', 'Expiry Date', \\\n",
        "                  'Ad Duration (Days)','Days before expiry', 'Link']\n",
        "\n",
        "      j_data = self.soup_set(self.job, self.locs)  #defines a list of dictionaries\n",
        "      df = pd.DataFrame(j_data)          #coverts the list above to a dataframe\n",
        "      df= df[cols_ord]                   #reorders the columns\n",
        "      df.to_csv(self.file_name+'.csv')   #creates a jobs_data csv file from the dataframe\n",
        "\n",
        "      return df "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "manager_jobs = JobScraping('manager', file_name='managerial jobs')\n",
        "mngr_file = manager_jobs.the_file"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TJCUSW1PdHXY",
        "outputId": "04b287ab-4bfe-4584-d98e-25885ce1c208"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<bound method JobScraping._main_run of <__main__.JobScraping object at 0x7f9f9f937cd0>>\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "H8Lsgwq6HBDB",
        "qRw6Wd8rkmol",
        "AZTFv6xDrXq-",
        "fwT4L4H-vai8",
        "Z3b8WIqevfhk",
        "eFm7slOrvnwO",
        "X7f3iiIYvwjn"
      ],
      "provenance": [],
      "authorship_tag": "ABX9TyM5RxBFODclf6J6Mi36j98C",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}